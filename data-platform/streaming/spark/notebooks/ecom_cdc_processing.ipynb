{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c249e6-f835-4fb3-be63-1d08bf7355e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://890c235e753d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ecommerce CDC Processing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f734dc4d420>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Ecommerce CDC Processing\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config(\n",
    "        'spark.jars.packages',\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.6.0\"\n",
    "    )\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44652886-b8f5-4d24-80ba-5d3fca1ad5d6",
   "metadata": {},
   "source": [
    "## Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfab10ec-702b-4a1c-be72-542551c4492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Kafka servers: kafka1:9092\n",
      "üìã CDC topic: pg.public.customers\n",
      "‚úÖ Kafka stream is connected\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Connect to Kafka ---\n",
    "KAFKA_SERVERS = \"kafka1:9092\"\n",
    "CDC_TOPIC = \"pg.public.customers\"\n",
    "\n",
    "print(f\"üîó Kafka servers: {KAFKA_SERVERS}\")\n",
    "print(f\"üìã CDC topic: {CDC_TOPIC}\")\n",
    "\n",
    "kafka_stream = (\n",
    "    spark.read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVERS)\n",
    "    .option(\"subscribe\", CDC_TOPIC)\n",
    "    # .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Kafka stream is connected\")\n",
    "kafka_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91e85d0-8d1a-4f74-93e5-b411cd0e1371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|              topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     0|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     1|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     2|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     3|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     4|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     5|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     6|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     7|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     8|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     9|2025-08-10 11:08:...|            0|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|    10|2025-08-10 11:08:...|            0|\n",
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202d578-9b16-44af-bc80-4ba191c7cc7a",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ab841-8929-4695-969f-ac897e519b64",
   "metadata": {},
   "source": [
    "### Parse raw message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0581d154-e528-4e68-917f-843fedaf42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define UDF to decode byte array to string\n",
    "def decode_bytes(bytes_array):\n",
    "    if bytes_array is not None:\n",
    "        try:\n",
    "            # Convert byte array to Python bytes and decode as UTF-8\n",
    "            return bytes(bytes_array).decode('utf-8')\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "decode_udf = udf(decode_bytes, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef562bb-c9ef-4e0e-9d05-ca273a1d9ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+---------+--------------------+\n",
      "|                 key|               value|              topic|partition|offset|           timestamp|timestampType|  key_str|           value_str|\n",
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+---------+--------------------+\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     0|2025-08-10 11:08:...|            0| {\"id\":2}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     1|2025-08-10 11:08:...|            0| {\"id\":4}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     2|2025-08-10 11:08:...|            0| {\"id\":5}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     3|2025-08-10 11:08:...|            0| {\"id\":6}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     4|2025-08-10 11:08:...|            0| {\"id\":7}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     5|2025-08-10 11:08:...|            0| {\"id\":8}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     6|2025-08-10 11:08:...|            0| {\"id\":9}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     7|2025-08-10 11:08:...|            0|{\"id\":10}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     8|2025-08-10 11:08:...|            0|{\"id\":11}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|     9|2025-08-10 11:08:...|            0|{\"id\":12}|{\"before\":null,\"a...|\n",
      "|[7B 22 69 64 22 3...|[7B 22 62 65 66 6...|pg.public.customers|        0|    10|2025-08-10 11:08:...|            0| {\"id\":1}|{\"before\":null,\"a...|\n",
      "+--------------------+--------------------+-------------------+---------+------+--------------------+-------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# --- Parse binary raw message ---\n",
    "kafka_json_df = (\n",
    "     kafka_stream\n",
    "    .withColumn('key_str', decode_udf(col(\"key\")))\n",
    "    .withColumn('value_str', expr('cast(value as string)'))\n",
    ")\n",
    "\n",
    "kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43255d98-efdd-4858-a076-5b7c37db6cbc",
   "metadata": {},
   "source": [
    "### Extract JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d709c4b-51bc-48a5-94f8-943b2b9b27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, LongType, StructField\n",
    "\n",
    "# Schema for CDC value JSON (Debezium format)\n",
    "key_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "value_schema = StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"created_at\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"created_at\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"ts_ms\", LongType(), True),\n",
    "        StructField(\"schema\", StringType(), True),\n",
    "        StructField(\"table\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"op\", StringType(), True),\n",
    "    StructField(\"ts_ms\", LongType(), True)  # Transaction timestamp\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5437c51-83ae-47a8-a683-450e275ba07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "transformed_df = (\n",
    "    kafka_json_df\n",
    "    .withColumn(\"key_json\", from_json(col(\"key_str\"), key_schema))\n",
    "    .withColumn('value_json', from_json(col('value_str'), value_schema))\n",
    "    .drop('value', 'key')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd630629-9d6d-41ce-9b1e-b9f6be4e4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------+--------------------+-------------+---------+--------------------+--------+--------------------+\n",
      "|              topic|partition|offset|           timestamp|timestampType|  key_str|           value_str|key_json|          value_json|\n",
      "+-------------------+---------+------+--------------------+-------------+---------+--------------------+--------+--------------------+\n",
      "|pg.public.customers|        0|     0|2025-08-10 11:08:...|            0| {\"id\":2}|{\"before\":null,\"a...|     {2}|{null, {2, Bob, b...|\n",
      "|pg.public.customers|        0|     1|2025-08-10 11:08:...|            0| {\"id\":4}|{\"before\":null,\"a...|     {4}|{null, {4, BatchC...|\n",
      "|pg.public.customers|        0|     2|2025-08-10 11:08:...|            0| {\"id\":5}|{\"before\":null,\"a...|     {5}|{null, {5, BatchC...|\n",
      "|pg.public.customers|        0|     3|2025-08-10 11:08:...|            0| {\"id\":6}|{\"before\":null,\"a...|     {6}|{null, {6, BatchC...|\n",
      "|pg.public.customers|        0|     4|2025-08-10 11:08:...|            0| {\"id\":7}|{\"before\":null,\"a...|     {7}|{null, {7, BatchC...|\n",
      "|pg.public.customers|        0|     5|2025-08-10 11:08:...|            0| {\"id\":8}|{\"before\":null,\"a...|     {8}|{null, {8, BatchC...|\n",
      "|pg.public.customers|        0|     6|2025-08-10 11:08:...|            0| {\"id\":9}|{\"before\":null,\"a...|     {9}|{null, {9, BatchC...|\n",
      "|pg.public.customers|        0|     7|2025-08-10 11:08:...|            0|{\"id\":10}|{\"before\":null,\"a...|    {10}|{null, {10, Batch...|\n",
      "|pg.public.customers|        0|     8|2025-08-10 11:08:...|            0|{\"id\":11}|{\"before\":null,\"a...|    {11}|{null, {11, Batch...|\n",
      "|pg.public.customers|        0|     9|2025-08-10 11:08:...|            0|{\"id\":12}|{\"before\":null,\"a...|    {12}|{null, {12, Batch...|\n",
      "|pg.public.customers|        0|    10|2025-08-10 11:08:...|            0| {\"id\":1}|{\"before\":null,\"a...|     {1}|{null, {1, Alice,...|\n",
      "+-------------------+---------+------+--------------------+-------------+---------+--------------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5e91e-acaa-4684-9c6d-bf07da34ec28",
   "metadata": {},
   "source": [
    "### Handle CDC ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a86842b-519b-4049-9c68-000ddb1190b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+----------------+-------------+--------+\n",
      "| id|                name|               email|      created_at|     _version|_deleted|\n",
      "+---+--------------------+--------------------+----------------+-------------+--------+\n",
      "|  2|                 Bob|     bob@example.com|1754742098873040|1754824114822|       0|\n",
      "|  4|BatchCustomer_175...|batchcustomer_175...|1754804426378610|1754824114828|       0|\n",
      "|  5|BatchCustomer_175...|batchcustomer_175...|1754804426381457|1754824114829|       0|\n",
      "|  6|BatchCustomer_175...|batchcustomer_175...|1754804426383958|1754824114829|       0|\n",
      "|  7|BatchCustomer_175...|batchcustomer_175...|1754804426386369|1754824114830|       0|\n",
      "|  8|BatchCustomer_175...|batchcustomer_175...|1754804426389480|1754824114830|       0|\n",
      "|  9|BatchCustomer_175...|batchcustomer_175...|1754804426393612|1754824114831|       0|\n",
      "| 10|BatchCustomer_175...|batchcustomer_175...|1754804426396510|1754824114831|       0|\n",
      "| 11|BatchCustomer_175...|batchcustomer_175...|1754804426398986|1754824114831|       0|\n",
      "| 12|BatchCustomer_175...|batchcustomer_175...|1754804426401162|1754824114832|       0|\n",
      "|  1|               Alice|  alice1@example.com|1754742098873040|1754824114832|       0|\n",
      "+---+--------------------+--------------------+----------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# --- Extract fields & handle CDC ops\n",
    "# For create/update/read: Use 'after' + _version = ts_ms\n",
    "# For delete: Insert with null fields or skip (for ReplacingMergeTree, insert with higher _version to replace)\n",
    "cdc_df = transformed_df.select(\n",
    "    # ID: From after/before/key\n",
    "    when(col(\"value_json.op\").isin(\"c\", \"u\", \"r\"), col(\"value_json.after.id\"))\n",
    "    .when(col(\"value_json.op\") == \"d\", col(\"value_json.before.id\"))\n",
    "    .otherwise(col(\"key_json.id\")).alias(\"id\"),\n",
    "    \n",
    "    # Fields: From after for insert/update, null for delete\n",
    "    when(col(\"value_json.op\").isin(\"c\", \"u\", \"r\"), col(\"value_json.after.name\"))\n",
    "    .otherwise(lit(None)).alias(\"name\"),\n",
    "    \n",
    "    when(col(\"value_json.op\").isin(\"c\", \"u\", \"r\"), col(\"value_json.after.email\"))\n",
    "    .otherwise(lit(None)).alias(\"email\"),\n",
    "    \n",
    "    when(col(\"value_json.op\").isin(\"c\", \"u\", \"r\"), col(\"value_json.after.created_at\"))\n",
    "    .otherwise(lit(None)).alias(\"created_at\"),\n",
    "    \n",
    "    # _version: From ts_ms\n",
    "    col(\"value_json.ts_ms\").alias(\"_version\"),\n",
    "    \n",
    "    # _deleted: 0 for insert/update, 1 for delete\n",
    "    when(col(\"value_json.op\") == \"d\", lit(1))\n",
    "    .otherwise(lit(0)).alias(\"_deleted\")\n",
    ")\n",
    "\n",
    "cdc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbf321-c72f-4489-86e4-866734e6ec5b",
   "metadata": {},
   "source": [
    "## Write event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1ad73",
   "metadata": {},
   "source": [
    "### ClickHouse Table Schema\n",
    "\n",
    "First, create the table in ClickHouse:\n",
    "\n",
    "```sql\n",
    "-- Connect to ClickHouse and create database\n",
    "CREATE DATABASE IF NOT EXISTS ecommerce_analytics;\n",
    "\n",
    "-- Create ReplacingMergeTree table for CDC\n",
    "CREATE TABLE IF NOT EXISTS ecommerce_analytics.customers_cdc (\n",
    "    id Int32,\n",
    "    name Nullable(String),\n",
    "    email Nullable(String),\n",
    "    created_at Nullable(Int64),\n",
    "    _version Int64,  -- For ReplacingMergeTree versioning\n",
    "    _deleted UInt8   -- 0 = active, 1 = deleted\n",
    ") ENGINE = ReplacingMergeTree(_version)\n",
    "ORDER BY id;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f433ad-5f0e-4f90-9e38-ffc545fa0ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è ClickHouse URL: jdbc:clickhouse://clickhouse:8123/ecommerce_analytics\n",
      "üë§ User: default\n",
      "\n",
      "üìä Writing to console for debugging:\n",
      "+---+--------------------------+--------------------------------------+----------------+-------------+--------+\n",
      "|id |name                      |email                                 |created_at      |_version     |_deleted|\n",
      "+---+--------------------------+--------------------------------------+----------------+-------------+--------+\n",
      "|2  |Bob                       |bob@example.com                       |1754742098873040|1754824114822|0       |\n",
      "|4  |BatchCustomer_1754804426_1|batchcustomer_1754804426_1@example.com|1754804426378610|1754824114828|0       |\n",
      "|5  |BatchCustomer_1754804426_2|batchcustomer_1754804426_2@example.com|1754804426381457|1754824114829|0       |\n",
      "|6  |BatchCustomer_1754804426_3|batchcustomer_1754804426_3@example.com|1754804426383958|1754824114829|0       |\n",
      "|7  |BatchCustomer_1754804426_4|batchcustomer_1754804426_4@example.com|1754804426386369|1754824114830|0       |\n",
      "|8  |BatchCustomer_1754804426_5|batchcustomer_1754804426_5@example.com|1754804426389480|1754824114830|0       |\n",
      "|9  |BatchCustomer_1754804426_6|batchcustomer_1754804426_6@example.com|1754804426393612|1754824114831|0       |\n",
      "|10 |BatchCustomer_1754804426_7|batchcustomer_1754804426_7@example.com|1754804426396510|1754824114831|0       |\n",
      "|11 |BatchCustomer_1754804426_8|batchcustomer_1754804426_8@example.com|1754804426398986|1754824114831|0       |\n",
      "|12 |BatchCustomer_1754804426_9|batchcustomer_1754804426_9@example.com|1754804426401162|1754824114832|0       |\n",
      "|1  |Alice                     |alice1@example.com                    |1754742098873040|1754824114832|0       |\n",
      "+---+--------------------------+--------------------------------------+----------------+-------------+--------+\n",
      "\n",
      "\n",
      "üíæ Writing to ClickHouse...\n",
      "‚úÖ Data written to ClickHouse successfully!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cdc_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# --- Write to Console (for debugging) ---\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Writing to console for debugging:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mcdc_df\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m20\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- Write to ClickHouse ---\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cdc_df' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Write to ClickHouse ---\n",
    "CLICKHOUSE_URL = \"jdbc:clickhouse://clickhouse:8123/ecommerce_analytics\"\n",
    "CLICKHOUSE_USER = \"default\"\n",
    "CLICKHOUSE_PASSWORD = \"clickhouse123\"\n",
    "\n",
    "# ClickHouse connection properties\n",
    "clickhouse_properties = {\n",
    "    \"user\": CLICKHOUSE_USER,\n",
    "    \"password\": CLICKHOUSE_PASSWORD,\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "print(f\"üóÑÔ∏è ClickHouse URL: {CLICKHOUSE_URL}\")\n",
    "print(f\"üë§ User: {CLICKHOUSE_USER}\")\n",
    "\n",
    "# --- Write to Console (for debugging) ---\n",
    "print(\"\\nüìä Writing to console for debugging:\")\n",
    "cdc_df.show(20, truncate=False)\n",
    "\n",
    "# --- Write to ClickHouse ---\n",
    "try:\n",
    "    print(\"\\nüíæ Writing to ClickHouse...\")\n",
    "    \n",
    "    # Write to ClickHouse table (ReplacingMergeTree with _version)\n",
    "    (cdc_df\n",
    "     .write\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", CLICKHOUSE_URL)\n",
    "     .option(\"dbtable\", \"customers_cdc\")  # Table name in ClickHouse\n",
    "     .option(\"user\", CLICKHOUSE_USER)\n",
    "     .option(\"password\", CLICKHOUSE_PASSWORD)\n",
    "     .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\")\n",
    "     .mode(\"append\")  # Always append for CDC\n",
    "     .save()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Data written to ClickHouse successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing to ClickHouse: {e}\")\n",
    "    print(\"üìù Writing to console only...\")\n",
    "    cdc_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
