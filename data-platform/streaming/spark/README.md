# ğŸš€ Spark CDC Processing Platform

Production-ready Spark Streaming platform for Change Data Capture (CDC) processing from PostgreSQL via Kafka to ClickHouse.

## ğŸ“ **Code Structure**

```
data-platform/streaming/spark/
â”œâ”€â”€ apps/                     # Application entry points
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/              # Infrastructure clients (Kafka, ClickHouse)
â”‚   â”œâ”€â”€ transformations/     # Data processing logic
â”‚   â”œâ”€â”€ utils/               # Pure utility functions
â”‚   â”œâ”€â”€ schemas/             # Data structure definitions
â”‚   â”œâ”€â”€ config/              # Configuration management
â”‚   â””â”€â”€ jobs/                # Business logic orchestration
â”œâ”€â”€ scripts/                 # Deployment scripts
â””â”€â”€ notebooks/               # Development notebooks
```

## ğŸ¯ **Design Principles**

- **Separation of Concerns**: Each layer has distinct responsibilities
- **Reusability**: Infrastructure clients work across multiple jobs
- **Testability**: Each component can be unit tested independently
- **Production Ready**: Comprehensive logging, error handling, and monitoring

## ğŸ—ï¸ **Components**

### **Common Layer** - Infrastructure Components
- `kafka_client.py`: KafkaReader for streaming/batch operations
- `clickhouse_client.py`: ClickHouseWriter for batch/streaming writes

### **Transformations Layer** - Data Processing
- `kafka_parser.py`: Parse Kafka messages (binary â†’ JSON)
- `cdc_transformer.py`: Transform CDC operations (INSERT/UPDATE/DELETE)

### **Other Layers**
- **Utils**: Stateless utility functions
- **Schemas**: Spark SQL schemas for type safety
- **Config**: Environment-aware configuration
- **Jobs**: Business logic orchestration

## ï¿½ **Quick Start**

### **Single Command Deployment**
```bash
# Run CDC job
make cdc-run
```

### **Development**
```bash
# Local development
python apps/run_cdc_job.py --job-type customers --debug

# Jupyter notebook
jupyter lab notebooks/ecom_cdc_processing.ipynb
```

### **Production**
```bash
# Spark submit
./scripts/submit_job.sh --job-type customers --master yarn
```

### Sample code flow
```txt
config â†’ I/O â†’ schema â†’ transformations â†’ job orchestration
```
Components:
1. Job entrypoint â†’ lives in apps/.
2. Job orchestration â†’ in jobs/, extends BaseJob.
3. I/O connectors â†’ in io/ (Kafka, ClickHouse, DB, etc).
4. Schemas â†’ in schemas/ (clear contracts, no inferSchema).
5. Transformations â†’ in transformations/, stateless & testable.
6. Utils & logging â†’ in utils/.
7. Config â†’ in config/, always externalized (YAML/env).

This pattern ensures separation of concerns, maintainability, and scalability for Spark jobs in production.

Common coding flow:

```md
Config

    Load environment configs (dev/staging/prod).
    Example: Kafka topic, DB URL, checkpoint path, batch intervalâ€¦
    ğŸ‘‰ Centralize in config/app_config.py.

I/O (Input)

    Read data from source (Kafka, S3, DB, HDFS).
    ğŸ‘‰ Put all connectors in io/ (e.g. kafka_client.py).

Schema

    Apply predefined schema (StructType) to raw input.
    Avoid inferSchema in production (not safe).
    ğŸ‘‰ Define in schemas/.

Transformations

    Business logic (cleaning, enrichment, aggregations).
    Keep them stateless, testable, reusable.
    ğŸ‘‰ Store in transformations/.

I/O (Output)

    Write results to sink (ClickHouse, S3, Parquet, etc).
    ğŸ‘‰ Again via io/ layer.

Job orchestration

    Glue all above steps together in jobs/.
    Job = Config + I/O (in) + Schema + Transformations + I/O (out).
    ğŸ‘‰ Example: OrdersJob in jobs/orders_job.py.
```
## ğŸ“Š **Monitoring**

- **Spark UI**: `http://driver:4040`
- **Key Metrics**: Input rate, processing latency, error rate, checkpoint duration

## ğŸ”® **Roadmap**

**Current**: âœ… Customers CDC, configuration management, Docker deployment

**Next**: ğŸ”„ Orders/Products CDC, monitoring dashboard, alerting

**Future**: ğŸ“‹ Schema registry, advanced error recovery, multi-sink support
