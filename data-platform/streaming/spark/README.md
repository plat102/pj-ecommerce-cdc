# ğŸš€ Spark CDC Processing Platform

Production-ready Spark Streaming platform for Change Data Capture (CDC) processing from PostgreSQL via Kafka to ClickHouse.

## ğŸ“ **Code Structure**

```
data-platform/streaming/spark/
â”œâ”€â”€ apps/                     # Application entry points
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/              # Infrastructure clients (Kafka, ClickHouse)
â”‚   â”œâ”€â”€ transformations/     # Data processing logic
â”‚   â”œâ”€â”€ utils/               # Pure utility functions
â”‚   â”œâ”€â”€ schemas/             # Data structure definitions
â”‚   â”œâ”€â”€ config/              # Configuration management
â”‚   â””â”€â”€ jobs/                # Business logic orchestration
â”œâ”€â”€ scripts/                 # Deployment scripts
â””â”€â”€ notebooks/               # Development notebooks
```

## ğŸ¯ **Design Principles**

- **Separation of Concerns**: Each layer has distinct responsibilities
- **Reusability**: Infrastructure clients work across multiple jobs
- **Testability**: Each component can be unit tested independently
- **Production Ready**: Comprehensive logging, error handling, and monitoring

## ğŸ—ï¸ **Components**

### **Common Layer** - Infrastructure Components
- `kafka_client.py`: KafkaReader for streaming/batch operations
- `clickhouse_client.py`: ClickHouseWriter for batch/streaming writes

### **Transformations Layer** - Data Processing
- `kafka_parser.py`: Parse Kafka messages (binary â†’ JSON)
- `cdc_transformer.py`: Transform CDC operations (INSERT/UPDATE/DELETE)

### **Other Layers**
- **Utils**: Stateless utility functions
- **Schemas**: Spark SQL schemas for type safety
- **Config**: Environment-aware configuration
- **Jobs**: Business logic orchestration

## ï¿½ **Quick Start**

### **Single Command Deployment**
```bash
# Run CDC job
make cdc-run
```

### **Development**
```bash
# Local development
python apps/run_cdc_job.py --job-type customers --debug

# Jupyter notebook
jupyter lab notebooks/ecom_cdc_processing.ipynb
```

### **Production**
```bash
# Spark submit
./scripts/submit_job.sh --job-type customers --master yarn
```

## ğŸ“Š **Monitoring**

- **Spark UI**: `http://driver:4040`
- **Key Metrics**: Input rate, processing latency, error rate, checkpoint duration

## ğŸ”® **Roadmap**

**Current**: âœ… Customers CDC, configuration management, Docker deployment

**Next**: ğŸ”„ Orders/Products CDC, monitoring dashboard, alerting

**Future**: ğŸ“‹ Schema registry, advanced error recovery, multi-sink support
